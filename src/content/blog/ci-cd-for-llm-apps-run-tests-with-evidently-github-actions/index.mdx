---
title: "CI/CD for LLM apps: Run tests with Evidently + GitHub actions"
description: "What happens when you tweak your prompt, switch model versions, or update the toolchain for your LLM agent â€“ will the answers get better or worse? You wouldn't..."
date: "2025-07-01"
slug: "ci-cd-for-llm-apps-run-tests-with-evidently-github-actions"
image: "./cover.jpg"
tags: []
---

What happens when you tweak your prompt, switch model versions, or update the toolchain for your LLM agent â€“ will the answers get better or worse? You wouldn't merge backend code without running tests. You shouldn't ship LLM code or prompt changes without validating output quality, either. Now you donâ€™t have to. We just released a GitHub Action that lets you automatically test your LLM application outputsÂ  â€“ every time you push code. It runs as part of your CI workflow, using the Evidently open-source library and (optionally) Evidently Cloud. Letâ€™s walk through what it does and how to use it.

## ğŸ¤– Why test LLM outputs?

Developing LLM apps means constant iteration. You:

*   Refactor the agent logic
*   Adjust system prompts
*   Swap a model or tool
*   Try a few â€œquickâ€ fixesâ€¦

But even tiny changes can produce regressions: less helpful responses, shorter or longer completions, or weird tone shifts. And theyâ€™re often silent â€“ your code checks pass, but your LLM behavior changes. By running tests on your LLM or agent's outputs â€“ not just your functions â€“ you can catch these changes early.

![LLM regression testing](./image-0.png)

Regression testing for LLM apps is one of the keyÂ [LLM evaluation workflows](https://www.evidentlyai.com/llm-guide/llm-evaluation). In this approach, you run evaluations on a pre-built test dataset to check if your AI system or agent still behaves as expected. There are two common ways to do this:

*   **Reference-based evaluations:**Â compare the generated responses against expected ground truth answers.
*   **Reference-free evaluations:**Â provide a set of test inputs, then automatically assess specific qualities of the responsesÂ  â€“ such as helpfulness, tone, correctness, or length.

Think of it as unit testing â€“ but for your LLM systemâ€™s behavior. And because language models are non-deterministic and designed to handle diverse, open-ended inputs, theyâ€™re best evaluated using structuredÂ **test datasets**Â rather than isolated test cases.

![Reference-based LLM evals](./image-1.png)

_Reference-based evals: comparing responses against expected ones._